{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_summary import DataFrameSummary\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype\n",
    "import IPython\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import imageio\n",
    "import matplotlib\n",
    "import cv2\n",
    "import shutil\n",
    "import glob\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_artifact\n",
    "from  mlflow.tracking import MlflowClient\n",
    "import mlflow.keras\n",
    "# import fastai\n",
    "# # fastai.__version__ # '1.0.58'\n",
    "# from fastai.imports import *\n",
    "# from fastai.structured import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "le = preprocessing.LabelEncoder()\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import time, sys\n",
    "import graphviz\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D\n",
    "from keras.layers.core import Activation,Dense,Dropout\n",
    "from keras.layers import Input, Flatten, GlobalAveragePooling2D, concatenate\n",
    "from keras.optimizers import Adam, RMSprop, Adamax, SGD\n",
    "from keras.utils import to_categorical, Sequence\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import MobileNet, VGG16, resnet50\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "from keras import models, layers, optimizers\n",
    "\n",
    "# from pyimagesearch import datasets\n",
    "# from pyimagesearch import models\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundus Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_dir = \"/ds/notebooks/patra/\"\n",
    "DATA_dir = \"/ds2/data/retina/UK_BB/patra/\"\n",
    "tts_path = DATA_dir+'TrainTestVal/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset consist of 85730 patients data with 6251 features for each patient\n"
     ]
    }
   ],
   "source": [
    "fundus_df = pd.read_feather('/ds2/data/retina/UK_BB/UK_BB_DATA/fundus.feather')\n",
    "# Checking data size\n",
    "n_patients = len(fundus_df)\n",
    "n_features = len(fundus_df.columns)\n",
    "print(\"Dataset consist of %d patients data with %d features for each patient\" %(n_patients, n_features))\n",
    "\n",
    "df_fundus_left = fundus_df.iloc[:,5613:5617] # fundus images of left eye\n",
    "df_fundus_right = fundus_df.iloc[:,5617:5621] # fundus images of right eye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function definitions executed\n"
     ]
    }
   ],
   "source": [
    "# Converting the disease code values to integer\n",
    "def conv2int(tempdf):\n",
    "    float_col = tempdf.select_dtypes(include = ['float64']) # This will select float columns only\n",
    "    for col in float_col.columns.values:\n",
    "        tempdf[col] = tempdf[col].fillna(0.0).astype('int64')\n",
    "    return tempdf\n",
    "\n",
    "#Getting unique items and their frequency\n",
    "def get_count(data):\n",
    "    data_freq = []\n",
    "    data_unique = []\n",
    "    for i in range(data.shape[0]):\n",
    "        uniqueValues, occurCount = np.unique(data[i], return_counts=True)\n",
    "        data_freq.append([uniqueValues, occurCount]) # for each patient\n",
    "        data_unique.append(uniqueValues.tolist()) # unique disease for each patient\n",
    "    return data_freq, data_unique\n",
    "\n",
    "# Obtaining unique disease occurance for each patient in a sorted array\n",
    "def get_sorted_unique(df):\n",
    "    df_int = conv2int(df)\n",
    "    arr = df_int.to_numpy()\n",
    "    data_freq, data_unique = get_count(arr)\n",
    "    uniqueValues, occurCount = np.unique(np.asarray([item for sublist in data_unique for item in sublist]), return_counts=True)\n",
    "    disease_freq = {uniqueValues[i]: occurCount[i] for i in range(len(occurCount))}\n",
    "    sort_disease_freq = sorted(disease_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sort_disease_freq\n",
    "\n",
    "# Displaying all values in a dataframe\n",
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\",1000):\n",
    "        with pd.option_context(\"display.max_columns\",1000):\n",
    "            display(df)\n",
    "\n",
    "            \n",
    "# To categorise the diseases into 4 categories as per disease_types dictionary\n",
    "def categorize(sort_disease_freq):\n",
    "    dis_cat = []\n",
    "    for i in range(len(sort_disease_freq)):\n",
    "        flag = False\n",
    "        for j in range(len(k)):\n",
    "    #         print(sort_disease_freq[i][0], v[j])\n",
    "            if(sort_disease_freq[i][0] in v[j]):\n",
    "                flag = True\n",
    "                flagv = j\n",
    "            else:\n",
    "                continue\n",
    "        if (flag == True):\n",
    "            dis_cat.append(k[flagv])\n",
    "        else:\n",
    "            dis_cat.append(\"Others\")\n",
    "            \n",
    "    return dis_cat\n",
    "\n",
    "def which_eye(patient):\n",
    "    s = df_100515.iloc[patient]\n",
    "    t = s.notna()\n",
    "    f = s[t==True]\n",
    "    lf = len(f)\n",
    "    if(lf==1):\n",
    "        val = f[0]\n",
    "    elif(lf>1):\n",
    "#         print(\"which_eye: Multiple answers for patient:\",patient,\"-\",lf)\n",
    "#         print(f)\n",
    "        val = f[lf-1]\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "\n",
    "def get_img_name(patient,df):\n",
    "    s = df.iloc[patient]\n",
    "    t = s.notna()\n",
    "    f = s[t==True]\n",
    "    lf = len(f)\n",
    "    if(lf==1):\n",
    "        txt = f[0]\n",
    "    elif(lf>1):\n",
    "#         print(\"get_img_name: Multiple answers for patient:\",patient,\"-\",lf)\n",
    "#         print(f)\n",
    "        txt = f[lf-1]\n",
    "    else:\n",
    "        txt = 'None'\n",
    "    prefix = fundus_df.iloc[patient]['eid']\n",
    "    img_name = str(prefix) + '_' + txt + '.png'\n",
    "        \n",
    "    return img_name    \n",
    "\n",
    "# convert image to array of size \"img_size\" and normalize by diving each pixel by 255\n",
    "def img2arr(img, img_size):\n",
    "    imgarr = cv2.imread(img)\n",
    "    imgarr = cv2.resize(imgarr,(img_size,img_size), interpolation=cv2.INTER_CUBIC)\n",
    "    imgarr = cv2.cvtColor(imgarr, cv2.COLOR_BGR2RGB)\n",
    "    return imgarr\n",
    "\n",
    "\n",
    "# To run: update_progress(iter/total no. of elements)\n",
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)\n",
    "    \n",
    "\n",
    "## GROUND TRUTH CREATION:\n",
    "\n",
    "def GT_100523():\n",
    "    GT_100523 = [0] * len(fundus_df)\n",
    "    # 1: Glaucoma\n",
    "    # 0: No Glaucoma\n",
    "\n",
    "    cnt = 0\n",
    "    cnt1 = 0\n",
    "    cnt2 = 0\n",
    "    for i in range(len(fundus_df)):\n",
    "        if(2 in df100523_unique_arr[i]): # 2 means Glaucoma\n",
    "            GT_100523[i] = 1\n",
    "            cnt+=1\n",
    "        elif(-1 in df100523_unique_arr[i] or -3 in df100523_unique_arr[i]): # -1 don't know, -3 prefer not to answer\n",
    "            GT_100523[i] = 99\n",
    "            cnt1+=1\n",
    "        else:\n",
    "            cnt2+=1\n",
    "\n",
    "    print(\"In DC 100523 : \\n\")\n",
    "    print(\"No. of patients having glaucoma : \",cnt)\n",
    "    print(\"Patients who don't know or prefer not to answer: \",cnt1)\n",
    "    print(\"Healthy patients:\", cnt2)\n",
    "    \n",
    "    return GT_100523\n",
    "\n",
    "def GT_19():\n",
    "    GT_19 = [0] * len(fundus_df)\n",
    "    # 1: Glaucoma\n",
    "    # 0: No Glaucoma\n",
    "\n",
    "    cnt = 0\n",
    "    for i in range(len(fundus_df)):\n",
    "        if (len(df19_clean_arr[i])>1): # len=1 contains nan only \n",
    "            for j in range(len(df19_clean_arr[i])):\n",
    "                if(('H40' in df19_clean_arr[i][j]) or ('H42' in df19_clean_arr[i][j])): # H40 and H42 refers to glaucoma related patients\n",
    "    #                 print(i, df19_clean_arr[i])\n",
    "                    GT_19[i] = 1 # 1 is assigned to those having glaucoma as per code 100523 and icd10\n",
    "                    cnt+=1\n",
    "                    break\n",
    "\n",
    "    print(\"No. of patients having glaucoma as per DC19 / ICD10 are: \",cnt)\n",
    "    return GT_19\n",
    "\n",
    "def GT_intersect():\n",
    "    GT_intersect = [0] * len(fundus_df)\n",
    "\n",
    "    # 1: Glaucoma as per both\n",
    "    # 0: No Glaucoma as per both\n",
    "    # 2: glau_19_not100523\n",
    "    # 3: glau_100523_not19\n",
    "\n",
    "    for i in range(len(fundus_df)):\n",
    "        if(2 in df100523_unique_arr[i]): # 2 means Glaucoma\n",
    "            if (len(df19_clean_arr[i])>1): # len=1 contains nan only \n",
    "                for j in range(len(df19_clean_arr[i])):\n",
    "                    if(('H40' in df19_clean_arr[i][j]) or ('H42' in df19_clean_arr[i][j])): # H40 and H42 refers to glaucoma related patients\n",
    "                        # print(i, df100523_unique_arr[i],\"=+=+=\",df19_clean_arr[i])\n",
    "                        GT_intersect[i] = 1 # 1 is assigned to those having glaucoma as per code 100523 and icd10\n",
    "                        break\n",
    "                    # Rows mismatching in the datasets\n",
    "                    elif(j==len(df19_clean_arr[i])-1):\n",
    "                        GT_intersect[i] = 3\n",
    "            else:\n",
    "                GT_intersect[i] = 3\n",
    "        else:\n",
    "            # Rows mismatching in the datasets\n",
    "            if(len(df19_clean_arr[i])>1): # len=1 contains nan only \n",
    "                for j in range(len(df19_clean_arr[i])):\n",
    "                    if(('H40' in df19_clean_arr[i][j]) or ('H42' in df19_clean_arr[i][j])): # H40 and H42 refers to glaucoma related patients\n",
    "                        GT_intersect[i] = 2 # not in 100523, but in icd10\n",
    "                        break\n",
    "\n",
    "    glau_100523_19 = []\n",
    "    no_glau_100523_19 = []\n",
    "    glau_100523_not19 = []\n",
    "    glau_19_not100523 = []\n",
    "\n",
    "    for i in range(len(GT_intersect)):\n",
    "        if(GT_intersect[i]==0):\n",
    "            no_glau_100523_19.append(i)\n",
    "        elif(GT_intersect[i]==1):\n",
    "            glau_100523_19.append(i)\n",
    "        elif(GT_intersect[i]==2):\n",
    "            glau_19_not100523.append(i)\n",
    "        elif(GT_intersect[i]==3):\n",
    "            glau_100523_not19.append(i)\n",
    "\n",
    "    print(\"Glaucoma in both:\",len(glau_100523_19),\"\\nGlaucoma in none:\",len(no_glau_100523_19),\n",
    "          \"\\nGlaucoma in 100523 only:\",len(glau_100523_not19),\"\\nGlaucoma in icd10 only:\",len(glau_19_not100523))\n",
    "    \n",
    "    return GT_intersect\n",
    "\n",
    "def GT_union():\n",
    "    GT_union = [0] * len(fundus_df)\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(len(fundus_df)):\n",
    "        if(2 in df100523_unique_arr[i]):\n",
    "            GT_union[i] = 1\n",
    "            cnt+=1\n",
    "        else:\n",
    "            if(len(df19_clean_arr[i])>1): # len=1 contains nan only \n",
    "                for j in range(len(df19_clean_arr[i])):\n",
    "                    if(('H40' in df19_clean_arr[i][j]) or ('H42' in df19_clean_arr[i][j])): # H40 and H42 refers to glaucoma related patients\n",
    "                        GT_union[i] = 1\n",
    "                        cnt+=1\n",
    "                        break\n",
    "                    elif(j == (len(df19_clean_arr[i])-1)):\n",
    "                        if(-1 in df100523_unique_arr[i] or -3 in df100523_unique_arr[i]): # -1 don't know, -3 prefer not to answer\n",
    "                            GT_union[i] = 99\n",
    "                         \n",
    "    print(\"No. of patients having glaucoma as per union of 100523 and ICD10 are: \",cnt)\n",
    "    \n",
    "    return GT_union\n",
    "\n",
    "def GT_list2df_merge(gt1,gt2,gt3,gt4):\n",
    "    # Convert Ground truths to dataframe\n",
    "    gt1_df = pd.DataFrame(gt1, columns=['GT_100523']) \n",
    "    gt2_df = pd.DataFrame(gt2, columns=['GT_19']) \n",
    "    gt3_df = pd.DataFrame(gt3, columns=['GT_intersect']) \n",
    "    gt4_df = pd.DataFrame(gt4, columns=['GT_union'])\n",
    "\n",
    "    allGT_df = pd.concat([gt1_df, gt2_df, gt3_df, gt4_df], axis=1)\n",
    "    \n",
    "    return allGT_df\n",
    "\n",
    "## FEATURE IMPORTANCE RELATED\n",
    "\n",
    "def proc_df(orig_df,orig_y):\n",
    "    '''\n",
    "    Separates the dataframe into dependent df and independent y variables\n",
    "    '''\n",
    "    flag = 0\n",
    "    df = orig_df.copy()\n",
    "    y = df[orig_y].values\n",
    "    df.drop(orig_y, axis=1, inplace=True)\n",
    "    return df,y\n",
    "# df, y = proc_df(fundus_gt,'Glaucoma_GT')\n",
    "\n",
    "def rmse(x,y):\n",
    "    return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(rf, train_x, val_x, train_y, val_y):\n",
    "    res = [rf.score(train_x, train_y), \n",
    "           rmse(rf.predict(train_x),train_y), \n",
    "           rf.score(val_x, val_y), \n",
    "           rmse(rf.predict(val_x),val_y)]\n",
    "    if hasattr(rf, 'oob_score_'):\n",
    "        res.append(rf.oob_score_)\n",
    "    print(res)\n",
    "    \n",
    "# copied from fastai.structured\n",
    "def draw_tree(t, df, size=10, ratio=0.6, precision=0):\n",
    "    \"\"\" Draws a representation of a random forest in IPython.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    t: The tree you wish to draw\n",
    "    df: The data used to train the tree. This is used to get the names of the features.\n",
    "    \"\"\"\n",
    "    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n",
    "                      special_characters=True, rotate=True, precision=precision)\n",
    "    IPython.display.display(graphviz.Source(re.sub('Tree {',\n",
    "       f'Tree {{ size={size}; ratio={ratio}', s)))\n",
    "\n",
    "# copied from fastai.structured\n",
    "# fi = rf_feat_importance(RF,enc_df) -- NOT USED\n",
    "def rf_feat_importance(m, df):\n",
    "    return pd.DataFrame({'cols':df.columns, 'descrip':df.columns.to_series().map(feat_desc_dict),'importance':m.feature_importances_}\n",
    "                       ).sort_values('importance', ascending=False)\n",
    "def plot_fi(finew):\n",
    "    return finew.plot('Features','Importance','barh',figsize=(15,15),legend=False)\n",
    "\n",
    "## DESCRIPTION OF FEATURES - DICTIONARY\n",
    "def feature_desc():\n",
    "    feat_desc_dict = dict([(k,v) for k,v in zip(feat_desc.UDI,feat_desc.Description)])  \n",
    "    feat_unique_val = []\n",
    "    feat_unique_val.append(feat_desc_dict['eid'])\n",
    "    for i in range(1,len(feat_unique)):\n",
    "        for key, value in feat_desc_dict.items():   # iter on both keys and values\n",
    "            if key.startswith(feat_unique[i]+'-'):\n",
    "                feat_unique_val.append(value)\n",
    "                break\n",
    "    feat_unique_val = np.asarray(feat_unique_val)\n",
    "    feat_unique_dict = dict(zip(feat_unique, feat_unique_val))\n",
    "    \n",
    "    return feat_desc_dict, feat_unique_dict\n",
    "\n",
    "## IMPORTANCE OF FEATURES - DICTIONARY\n",
    "def feature_imp(RF, enc_df):    \n",
    "    imp = RF.feature_importances_.tolist()\n",
    "    col = enc_df.columns.values.tolist()\n",
    "    feat_imp_dict = dict(zip(col, imp))\n",
    "\n",
    "    feat_unique_impval = []\n",
    "    if(feat_unique[0] in feat_imp_dict.keys()):\n",
    "        feat_unique_impval.append(feat_imp_dict[feat_unique[0]])\n",
    "    else:\n",
    "        feat_unique_impval.append(0)\n",
    "\n",
    "    for i in range(1,len(feat_unique)):\n",
    "        temp = []\n",
    "        for key, value in feat_imp_dict.items():   # iter on both keys and values\n",
    "            if key.startswith(feat_unique[i]+'-'):\n",
    "                temp.append(feat_imp_dict[key])\n",
    "        if(len(temp) > 0):\n",
    "            feat_unique_impval.append(max(temp))\n",
    "        else:\n",
    "            feat_unique_impval.append(0)\n",
    "\n",
    "    feat_unique_imp_dict = dict(zip(feat_unique, feat_unique_impval))\n",
    "    \n",
    "    return feat_imp_dict, feat_unique_imp_dict\n",
    "\n",
    "def get_imp_features(num): # returns the dataframe of features along with their importances\n",
    "    feat = []\n",
    "    imp = []\n",
    "    imp_sorted = sorted(feat_unique_imp_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    for i in range(num):\n",
    "        feat.append(feat_unique_dict[imp_sorted[i][0]])\n",
    "        imp.append(imp_sorted[i][1])\n",
    "    \n",
    "    return pd.DataFrame(list(zip(feat, imp)), \n",
    "               columns =['Features', 'Importance'])\n",
    "\n",
    "\n",
    "\n",
    "# Plot the training and validation loss + accuracy\n",
    "def plot_training(history):\n",
    "    # summarize history for accuracy  \n",
    "\n",
    "    plt.subplot(211)  \n",
    "    plt.plot(history.history['accuracy'])  \n",
    "    plt.plot(history.history['val_accuracy'])  \n",
    "    plt.title('model accuracy')  \n",
    "    plt.ylabel('accuracy')  \n",
    "    plt.xlabel('epoch')  \n",
    "    plt.legend(['train', 'test'], loc='upper left')  \n",
    "\n",
    "    # summarize history for loss  \n",
    "\n",
    "    plt.subplot(212)  \n",
    "    plt.plot(history.history['loss'])  \n",
    "    plt.plot(history.history['val_loss'])  \n",
    "    plt.title('model loss')  \n",
    "    plt.ylabel('loss')  \n",
    "    plt.xlabel('epoch')  \n",
    "    plt.legend(['train', 'test'], loc='upper left')  \n",
    "    plt.show()  \n",
    "    \n",
    "\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] \n",
    "\n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > p)\n",
    "    normed_cm = (cm.T / cm.astype(np.float).sum(axis=1)).T\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(normed_cm, annot=True, fmt=\"f\",cbar=True, cmap=\"Blues\")\n",
    "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    print('Healthy Patients - correctly identified (True Negatives): ', cm[0][0], normed_cm[0][0])\n",
    "    print('Healthy Patients - predicted to have glaucoma (False Positives): ', cm[0][1], normed_cm[0][1])\n",
    "    print('Glaucoma Patients - predicted healthy (False Negatives): ', cm[1][0], normed_cm[1][0])\n",
    "    print('Glaucoma Patients - correctly identified (True Positives): ', cm[1][1], normed_cm[1][1])\n",
    "    print('\\nTotal Glaucoma patients: ', np.sum(cm[1]), np.sum(normed_cm[1]))\n",
    "    print('Total Healthy patients: ', np.sum(cm[0]), np.sum(normed_cm[0]))\n",
    "\n",
    "\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    heatmap = sns.heatmap(df_cm, annot=True, fmt=\"f\", cmap=\"Blues\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def plot_metrics(history):\n",
    "    metrics =  ['loss','Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n",
    "    \n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        \n",
    "        plt.subplot(3,2,n+1)\n",
    "        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "                 color=colors[0], linestyle=\"--\", label='Validation')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "#         if metric == 'loss':\n",
    "#             plt.ylim([0, plt.ylim()[1]])\n",
    "#         else:\n",
    "        plt.ylim([0,1])\n",
    "        plt.legend()\n",
    "        \n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = roc_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "def plot_loss(history, label, n):\n",
    "    # Use a log scale to show the wide range of values.\n",
    "    plt.semilogy(history.epoch,  history.history['loss'],\n",
    "               color=colors[n], label='Train '+label)\n",
    "    plt.semilogy(history.epoch,  history.history['val_loss'],\n",
    "          color=colors[n], label='Val '+label,\n",
    "          linestyle=\"--\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "def get_ROI(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # Find contour and sort by contour area\n",
    "    cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    if(len(cnts)>0):\n",
    "        cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "        # Find bounding box and extract ROI\n",
    "        for c in cnts:\n",
    "            x,y,w,h = cv2.boundingRect(c)\n",
    "            ROI = image[y:y+h, x:x+w]\n",
    "            break\n",
    "    else:\n",
    "        ROI = image\n",
    "        \n",
    "    return ROI\n",
    "\n",
    "def Recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def Precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def F1(y_true, y_pred):\n",
    "    precision = Precision(y_true, y_pred)\n",
    "    recall = Recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def get_metrics(y_train, y_val, y_test, train_pred, val_pred, test_pred):\n",
    "    print(\"Metrics:\\t\\tTrain:\\t\\t\\tValidation:\\t\\tTest:\")\n",
    "    print(\"Accuracy:\\t\",accuracy_score(y_train, train_pred), \"\\t\", accuracy_score(y_val, val_pred), \"\\t\", accuracy_score(y_test, test_pred))\n",
    "    print(\"Recall:\\t\\t\",recall_score(y_train, train_pred), \"\\t\", recall_score(y_val, val_pred), \"\\t\", recall_score(y_test, test_pred))\n",
    "    print(\"Precision:\\t\",precision_score(y_train, train_pred), \"\\t\", precision_score(y_val, val_pred), \"\\t\", precision_score(y_test, test_pred))\n",
    "    print(\"F1 score:\\t\",f1_score(y_train, train_pred), \"\\t\", f1_score(y_val, val_pred), \"\\t\", f1_score(y_test, test_pred))\n",
    "\n",
    "    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, train_pred)\n",
    "    fpr_val, tpr_val, thresholds_val = roc_curve(y_val, val_pred)\n",
    "    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, test_pred)\n",
    "    print(\"AUC:\\t\\t\",auc(fpr_train, tpr_train), \"\\t\", auc(fpr_val, tpr_val), \"\\t\", auc(fpr_test, tpr_test))\n",
    "    \n",
    "def vgg_model():       \n",
    "    vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(red_size, red_size, 3))\n",
    "\n",
    "    # Freeze the layers except the last 4 layers\n",
    "    for layer in vgg_conv.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Create the model\n",
    "    model = models.Sequential()\n",
    "     # Add the vgg convolutional base model\n",
    "    model.add(vgg_conv)\n",
    " \n",
    "    # Add new layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu',kernel_regularizer=l2(reg_value)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    return model\n",
    "\n",
    "def resnet50_model():\n",
    "    resnet_model = resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=(red_size, red_size, 3))\n",
    "    # UnFreeze last conv layer\n",
    "    for layer in resnet_model.layers[:158]:\n",
    "        layer.trainable = False\n",
    "    for layer in resnet_model.layers[158:]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "     # Create the model\n",
    "    model = models.Sequential()\n",
    "    model.add(resnet_model)\n",
    " \n",
    "    # Add new layers\n",
    "    # https://adventuresinmachinelearning.com/global-average-pooling-convolutional-neural-networks/\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    return model\n",
    "\n",
    "def mobilenet_model():       \n",
    "    mobile_conv = MobileNet(weights='imagenet', include_top=False, input_shape=(red_size, red_size, 3))\n",
    "\n",
    "    # UnFreeze last conv layer\n",
    "    for layer in mobile_conv.layers[:78]:\n",
    "        layer.trainable = False\n",
    "    for layer in mobile_conv.layers[78:]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    # Create the model\n",
    "    model = models.Sequential()\n",
    "    model.add(mobile_conv)\n",
    " \n",
    "    # Add new layers\n",
    "    # https://adventuresinmachinelearning.com/global-average-pooling-convolutional-neural-networks/\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    return model\n",
    "\n",
    "def save_mlflow(keras_model_path,exp_name,which_gt,which_model,seq,BATCH_SIZE,n_epochs,n_steps_per_epoch,n_validation_steps,reg_value,learning_rate,decay,\n",
    "                opt,history,results_tr,results_vl,train_cm,val_cm):\n",
    "    # Create Experiment\n",
    "    exp_id = mlflow.create_experiment(exp_name)\n",
    "\n",
    "    # Start run as a child of that experiment\n",
    "    mlflow.start_run(experiment_id=exp_id)\n",
    "\n",
    "    # Save parameters\n",
    "    mlflow.log_param(\"which_gt\", which_gt)\n",
    "    mlflow.log_param(\"which_model\", which_model)\n",
    "    mlflow.log_param(\"augmentation\", seq)\n",
    "    mlflow.log_param(\"BATCH_SIZE\", BATCH_SIZE)\n",
    "    mlflow.log_param(\"epochs\", n_epochs)\n",
    "    mlflow.log_param(\"steps_per_epoch\", n_steps_per_epoch)\n",
    "    mlflow.log_param(\"validation_steps\", n_validation_steps)\n",
    "    mlflow.log_param(\"reg_value\", reg_value)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"decay\", decay)\n",
    "    mlflow.log_param(\"optimizer\", opt)\n",
    "    \n",
    "    # Save Model\n",
    "    mlflow.keras.save_model(model, keras_model_path)\n",
    "\n",
    "    # Save Metrics\n",
    "    for i in history.history.keys():\n",
    "        for j in range(len(history.history[i])):\n",
    "            mlflow.log_metric(i, history.history[i][j], step=j)\n",
    "\n",
    "    for i in ['results_tr','results_vl','train_cm','val_cm']:\n",
    "        for j in range(len(eval(i))):\n",
    "            mlflow.log_metric(i, eval(i)[j], step=j)\n",
    "            \n",
    "    mlflow.end_run()\n",
    "    \n",
    "def get_notebook_name():\n",
    "    from IPython.core.display import Javascript\n",
    "    from IPython.display import display\n",
    "\n",
    "    display(Javascript('Jupyter.notebook.kernel.execute(\\\"this_notebook = \" + \"\\'\"+Jupyter.notebook.notebook_name+\"\\'\");'))\n",
    "\n",
    "    # os.path.abspath(this_notebook)\n",
    "    return this_notebook\n",
    "\n",
    "\n",
    "print(\"All Function definitions executed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2020",
   "language": "python",
   "name": "python2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
