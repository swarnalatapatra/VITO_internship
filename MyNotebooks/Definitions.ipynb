{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting GPU Connections and Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-17 08:55:57.822981\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mthunder\u001b[m  Fri Jan 17 09:03:22 2020\r\n",
      "\u001b[36m[0]\u001b[m \u001b[34mGeForce GTX 1080 Ti\u001b[m |\u001b[31m 39'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m10767\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mhemelinr\u001b[m(\u001b[33m10757M\u001b[m)\r\n",
      "\u001b[36m[1]\u001b[m \u001b[34mGeForce GTX 1080 Ti\u001b[m |\u001b[31m 24'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m10767\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mhemelinr\u001b[m(\u001b[33m10757M\u001b[m)\r\n",
      "\u001b[36m[2]\u001b[m \u001b[34mGeForce GTX 1080 Ti\u001b[m |\u001b[31m 25'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m   10\u001b[m / \u001b[33m11178\u001b[m MB |\r\n",
      "\u001b[36m[3]\u001b[m \u001b[34mGeForce GTX 1080 Ti\u001b[m |\u001b[31m 24'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m10936\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mhemelinr\u001b[m(\u001b[33m10926M\u001b[m)\r\n",
      "\u001b[36m[4]\u001b[m \u001b[34mGeForce GTX 1080 Ti\u001b[m |\u001b[31m 23'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m10767\u001b[m / \u001b[33m11178\u001b[m MB | \u001b[1m\u001b[30mhemelinr\u001b[m(\u001b[33m10757M\u001b[m)\r\n",
      "\u001b[36m[5]\u001b[m \u001b[34mGeForce GTX 1080 Ti\u001b[m |\u001b[31m 26'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m   10\u001b[m / \u001b[33m11178\u001b[m MB |\r\n",
      "\u001b[36m[6]\u001b[m \u001b[34mGeForce GTX 1080 Ti\u001b[m |\u001b[31m 29'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m   10\u001b[m / \u001b[33m11178\u001b[m MB |\r\n",
      "\u001b[36m[7]\u001b[m \u001b[34mGeForce GTX 1080 Ti\u001b[m |\u001b[31m 27'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m   10\u001b[m / \u001b[33m11178\u001b[m MB |\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ds/environments/python2020/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n",
      "/ds/environments/python2020/lib/python3.7/site-packages/keras/callbacks/callbacks.py:19: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_summary import DataFrameSummary\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype\n",
    "import IPython\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import imageio\n",
    "import matplotlib\n",
    "import cv2\n",
    "import shutil\n",
    "import glob\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_artifact\n",
    "from  mlflow.tracking import MlflowClient\n",
    "import mlflow.keras\n",
    "# import fastai\n",
    "# # fastai.__version__ # '1.0.58'\n",
    "# from fastai.imports import *\n",
    "# from fastai.structured import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "le = preprocessing.LabelEncoder()\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import time, sys\n",
    "import graphviz\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D\n",
    "from keras.layers.core import Activation,Dense,Dropout\n",
    "from keras.layers import Input, Flatten, GlobalAveragePooling2D, concatenate\n",
    "from keras.optimizers import Adam, RMSprop, Adamax, SGD\n",
    "from keras.utils import to_categorical, Sequence\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import MobileNet, VGG16, resnet50\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "from keras import models, layers, optimizers\n",
    "\n",
    "# from pyimagesearch import datasets\n",
    "# from pyimagesearch import models\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_dir = \"/ds/notebooks/patra/\"\n",
    "DATA_dir = \"/ds2/data/retina/UK_BB/patra/\"\n",
    "tts_path = DATA_dir+'TrainTestVal/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset consist of 85730 patients data with 6251 features for each patient\n"
     ]
    }
   ],
   "source": [
    "fundus_df = pd.read_feather('/ds2/data/retina/UK_BB/UK_BB_DATA/fundus.feather')\n",
    "# Checking data size\n",
    "n_patients = len(fundus_df)\n",
    "n_features = len(fundus_df.columns)\n",
    "print(\"Dataset consist of %d patients data with %d features for each patient\" %(n_patients, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>eid</th>\n",
       "      <th>21-0.0</th>\n",
       "      <th>21-1.0</th>\n",
       "      <th>21-2.0</th>\n",
       "      <th>23-0.0</th>\n",
       "      <th>23-1.0</th>\n",
       "      <th>23-2.0</th>\n",
       "      <th>31-0.0</th>\n",
       "      <th>...</th>\n",
       "      <th>41210-0.76</th>\n",
       "      <th>41210-0.77</th>\n",
       "      <th>41210-0.78</th>\n",
       "      <th>41210-0.79</th>\n",
       "      <th>41210-0.80</th>\n",
       "      <th>41210-0.81</th>\n",
       "      <th>41210-0.82</th>\n",
       "      <th>41210-0.83</th>\n",
       "      <th>41210-0.84</th>\n",
       "      <th>41210-0.85</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1000132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1000277</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>1000472</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>1000487</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>1000684</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6251 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index      eid  21-0.0  21-1.0  21-2.0  23-0.0  23-1.0  23-2.0  \\\n",
       "0        0     12  1000132     1.0     NaN     NaN     0.0     NaN     NaN   \n",
       "1        1     26  1000277     1.0     NaN     NaN     0.0     NaN     NaN   \n",
       "2        2     46  1000472     1.0     NaN     NaN     0.0     NaN     NaN   \n",
       "3        3     47  1000487     1.0     NaN     NaN     0.0     NaN     NaN   \n",
       "4        4     67  1000684     1.0     NaN     1.0     0.0     NaN     0.0   \n",
       "\n",
       "   31-0.0  ...  41210-0.76  41210-0.77  41210-0.78  41210-0.79  41210-0.80  \\\n",
       "0       1  ...        None        None        None        None        None   \n",
       "1       1  ...        None        None        None        None        None   \n",
       "2       0  ...        None        None        None        None        None   \n",
       "3       1  ...        None        None        None        None        None   \n",
       "4       1  ...        None        None        None        None        None   \n",
       "\n",
       "   41210-0.81  41210-0.82 41210-0.83  41210-0.84  41210-0.85  \n",
       "0        None        None       None        None        None  \n",
       "1        None        None       None        None        None  \n",
       "2        None        None       None        None        None  \n",
       "3        None        None       None        None        None  \n",
       "4        None        None       None        None        None  \n",
       "\n",
       "[5 rows x 6251 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fundus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fundus_left = fundus_df.iloc[:,5613:5617] # fundus images of left eye\n",
    "df_fundus_right = fundus_df.iloc[:,5617:5621] # fundus images of right eye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function definitions executed\n"
     ]
    }
   ],
   "source": [
    "# Converting the disease code values to integer\n",
    "def conv2int(tempdf):\n",
    "    float_col = tempdf.select_dtypes(include = ['float64']) # This will select float columns only\n",
    "    for col in float_col.columns.values:\n",
    "        tempdf[col] = tempdf[col].fillna(0.0).astype('int64')\n",
    "    return tempdf\n",
    "\n",
    "#Getting unique items and their frequency\n",
    "def get_count(data):\n",
    "    data_freq = []\n",
    "    data_unique = []\n",
    "    for i in range(data.shape[0]):\n",
    "        uniqueValues, occurCount = np.unique(data[i], return_counts=True)\n",
    "        data_freq.append([uniqueValues, occurCount]) # for each patient\n",
    "        data_unique.append(uniqueValues.tolist()) # unique disease for each patient\n",
    "    return data_freq, data_unique\n",
    "\n",
    "# Obtaining unique disease occurance for each patient in a sorted array\n",
    "def get_sorted_unique(df):\n",
    "    df_int = conv2int(df)\n",
    "    arr = df_int.to_numpy()\n",
    "    data_freq, data_unique = get_count(arr)\n",
    "    uniqueValues, occurCount = np.unique(np.asarray([item for sublist in data_unique for item in sublist]), return_counts=True)\n",
    "    disease_freq = {uniqueValues[i]: occurCount[i] for i in range(len(occurCount))}\n",
    "    sort_disease_freq = sorted(disease_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sort_disease_freq\n",
    "\n",
    "# Displaying all values in a dataframe\n",
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\",1000):\n",
    "        with pd.option_context(\"display.max_columns\",1000):\n",
    "            display(df)\n",
    "\n",
    "            \n",
    "# To categorise the diseases into 4 categories as per disease_types dictionary\n",
    "def categorize(sort_disease_freq):\n",
    "    dis_cat = []\n",
    "    for i in range(len(sort_disease_freq)):\n",
    "        flag = False\n",
    "        for j in range(len(k)):\n",
    "    #         print(sort_disease_freq[i][0], v[j])\n",
    "            if(sort_disease_freq[i][0] in v[j]):\n",
    "                flag = True\n",
    "                flagv = j\n",
    "            else:\n",
    "                continue\n",
    "        if (flag == True):\n",
    "            dis_cat.append(k[flagv])\n",
    "        else:\n",
    "            dis_cat.append(\"Others\")\n",
    "            \n",
    "    return dis_cat\n",
    "\n",
    "def which_eye(patient):\n",
    "    s = df_100515.iloc[patient]\n",
    "    t = s.notna()\n",
    "    f = s[t==True]\n",
    "    lf = len(f)\n",
    "    if(lf==1):\n",
    "        val = f[0]\n",
    "    elif(lf>1):\n",
    "#         print(\"which_eye: Multiple answers for patient:\",patient,\"-\",lf)\n",
    "#         print(f)\n",
    "        val = f[lf-1]\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "\n",
    "def get_img_name(patient,df):\n",
    "    s = df.iloc[patient]\n",
    "    t = s.notna()\n",
    "    f = s[t==True]\n",
    "    lf = len(f)\n",
    "    if(lf==1):\n",
    "        txt = f[0]\n",
    "    elif(lf>1):\n",
    "#         print(\"get_img_name: Multiple answers for patient:\",patient,\"-\",lf)\n",
    "#         print(f)\n",
    "        txt = f[lf-1]\n",
    "    else:\n",
    "        txt = 'None'\n",
    "    prefix = fundus_df.iloc[patient]['eid']\n",
    "    img_name = str(prefix) + '_' + txt + '.png'\n",
    "        \n",
    "    return img_name    \n",
    "\n",
    "# convert image to array of size \"img_size\" and normalize by diving each pixel by 255\n",
    "def img2arr(img, img_size):\n",
    "    imgarr = cv2.imread(img)\n",
    "    imgarr = cv2.resize(imgarr,(img_size,img_size), interpolation=cv2.INTER_CUBIC)\n",
    "    imgarr = cv2.cvtColor(imgarr, cv2.COLOR_BGR2RGB)\n",
    "    return imgarr\n",
    "\n",
    "\n",
    "# To run: update_progress(iter/total no. of elements)\n",
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)\n",
    "    \n",
    "\n",
    "## GROUND TRUTH CREATION:\n",
    "\n",
    "def GT_100523():\n",
    "    GT_100523 = [0] * len(fundus_df)\n",
    "    # 1: Glaucoma\n",
    "    # 0: No Glaucoma\n",
    "\n",
    "    cnt = 0\n",
    "    cnt1 = 0\n",
    "    cnt2 = 0\n",
    "    for i in range(len(fundus_df)):\n",
    "        if(2 in df100523_unique_arr[i]): # 2 means Glaucoma\n",
    "            GT_100523[i] = 1\n",
    "            cnt+=1\n",
    "        elif(-1 in df100523_unique_arr[i] or -3 in df100523_unique_arr[i]): # -1 don't know, -3 prefer not to answer\n",
    "            GT_100523[i] = 99\n",
    "            cnt1+=1\n",
    "        else:\n",
    "            cnt2+=1\n",
    "\n",
    "    print(\"In DC 100523 : \\n\")\n",
    "    print(\"No. of patients having glaucoma : \",cnt)\n",
    "    print(\"Patients who don't know or prefer not to answer: \",cnt1)\n",
    "    print(\"Healthy patients:\", cnt2)\n",
    "    \n",
    "    return GT_100523\n",
    "\n",
    "def GT_19():\n",
    "    GT_19 = [0] * len(fundus_df)\n",
    "    # 1: Glaucoma\n",
    "    # 0: No Glaucoma\n",
    "\n",
    "    cnt = 0\n",
    "    for i in range(len(fundus_df)):\n",
    "        if (len(df19_clean_arr[i])>1): # len=1 contains nan only \n",
    "            for j in range(len(df19_clean_arr[i])):\n",
    "                if(('H40' in df19_clean_arr[i][j]) or ('H42' in df19_clean_arr[i][j])): # H40 and H42 refers to glaucoma related patients\n",
    "    #                 print(i, df19_clean_arr[i])\n",
    "                    GT_19[i] = 1 # 1 is assigned to those having glaucoma as per code 100523 and icd10\n",
    "                    cnt+=1\n",
    "                    break\n",
    "\n",
    "    print(\"No. of patients having glaucoma as per DC19 / ICD10 are: \",cnt)\n",
    "    return GT_19\n",
    "\n",
    "def GT_intersect():\n",
    "    GT_intersect = [0] * len(fundus_df)\n",
    "\n",
    "    # 1: Glaucoma as per both\n",
    "    # 0: No Glaucoma as per both\n",
    "    # 2: glau_19_not100523\n",
    "    # 3: glau_100523_not19\n",
    "\n",
    "    for i in range(len(fundus_df)):\n",
    "        if(2 in df100523_unique_arr[i]): # 2 means Glaucoma\n",
    "            if (len(df19_clean_arr[i])>1): # len=1 contains nan only \n",
    "                for j in range(len(df19_clean_arr[i])):\n",
    "                    if(('H40' in df19_clean_arr[i][j]) or ('H42' in df19_clean_arr[i][j])): # H40 and H42 refers to glaucoma related patients\n",
    "                        # print(i, df100523_unique_arr[i],\"=+=+=\",df19_clean_arr[i])\n",
    "                        GT_intersect[i] = 1 # 1 is assigned to those having glaucoma as per code 100523 and icd10\n",
    "                        break\n",
    "                    # Rows mismatching in the datasets\n",
    "                    elif(j==len(df19_clean_arr[i])-1):\n",
    "                        GT_intersect[i] = 3\n",
    "            else:\n",
    "                GT_intersect[i] = 3\n",
    "        else:\n",
    "            # Rows mismatching in the datasets\n",
    "            if(len(df19_clean_arr[i])>1): # len=1 contains nan only \n",
    "                for j in range(len(df19_clean_arr[i])):\n",
    "                    if(('H40' in df19_clean_arr[i][j]) or ('H42' in df19_clean_arr[i][j])): # H40 and H42 refers to glaucoma related patients\n",
    "                        GT_intersect[i] = 2 # not in 100523, but in icd10\n",
    "                        break\n",
    "\n",
    "    glau_100523_19 = []\n",
    "    no_glau_100523_19 = []\n",
    "    glau_100523_not19 = []\n",
    "    glau_19_not100523 = []\n",
    "\n",
    "    for i in range(len(GT_intersect)):\n",
    "        if(GT_intersect[i]==0):\n",
    "            no_glau_100523_19.append(i)\n",
    "        elif(GT_intersect[i]==1):\n",
    "            glau_100523_19.append(i)\n",
    "        elif(GT_intersect[i]==2):\n",
    "            glau_19_not100523.append(i)\n",
    "        elif(GT_intersect[i]==3):\n",
    "            glau_100523_not19.append(i)\n",
    "\n",
    "    print(\"Glaucoma in both:\",len(glau_100523_19),\"\\nGlaucoma in none:\",len(no_glau_100523_19),\n",
    "          \"\\nGlaucoma in 100523 only:\",len(glau_100523_not19),\"\\nGlaucoma in icd10 only:\",len(glau_19_not100523))\n",
    "    \n",
    "    return GT_intersect\n",
    "\n",
    "def GT_union():\n",
    "    GT_union = [0] * len(fundus_df)\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(len(fundus_df)):\n",
    "        if(2 in df100523_unique_arr[i]):\n",
    "            GT_union[i] = 1\n",
    "            cnt+=1\n",
    "        else:\n",
    "            if(len(df19_clean_arr[i])>1): # len=1 contains nan only \n",
    "                for j in range(len(df19_clean_arr[i])):\n",
    "                    if(('H40' in df19_clean_arr[i][j]) or ('H42' in df19_clean_arr[i][j])): # H40 and H42 refers to glaucoma related patients\n",
    "                        GT_union[i] = 1\n",
    "                        cnt+=1\n",
    "                        break\n",
    "                    elif(j == (len(df19_clean_arr[i])-1)):\n",
    "                        if(-1 in df100523_unique_arr[i] or -3 in df100523_unique_arr[i]): # -1 don't know, -3 prefer not to answer\n",
    "                            GT_union[i] = 99\n",
    "                         \n",
    "    print(\"No. of patients having glaucoma as per union of 100523 and ICD10 are: \",cnt)\n",
    "    \n",
    "    return GT_union\n",
    "\n",
    "def GT_list2df_merge(gt1,gt2,gt3,gt4):\n",
    "    # Convert Ground truths to dataframe\n",
    "    gt1_df = pd.DataFrame(gt1, columns=['GT_100523']) \n",
    "    gt2_df = pd.DataFrame(gt2, columns=['GT_19']) \n",
    "    gt3_df = pd.DataFrame(gt3, columns=['GT_intersect']) \n",
    "    gt4_df = pd.DataFrame(gt4, columns=['GT_union'])\n",
    "\n",
    "    allGT_df = pd.concat([gt1_df, gt2_df, gt3_df, gt4_df], axis=1)\n",
    "    \n",
    "    return allGT_df\n",
    "\n",
    "## FEATURE IMPORTANCE RELATED\n",
    "\n",
    "def proc_df(orig_df,orig_y):\n",
    "    '''\n",
    "    Separates the dataframe into dependent df and independent y variables\n",
    "    '''\n",
    "    flag = 0\n",
    "    df = orig_df.copy()\n",
    "    y = df[orig_y].values\n",
    "    df.drop(orig_y, axis=1, inplace=True)\n",
    "    return df,y\n",
    "# df, y = proc_df(fundus_gt,'Glaucoma_GT')\n",
    "\n",
    "def rmse(x,y):\n",
    "    return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(rf, train_x, val_x, train_y, val_y):\n",
    "    res = [rf.score(train_x, train_y), \n",
    "           rmse(rf.predict(train_x),train_y), \n",
    "           rf.score(val_x, val_y), \n",
    "           rmse(rf.predict(val_x),val_y)]\n",
    "    if hasattr(rf, 'oob_score_'):\n",
    "        res.append(rf.oob_score_)\n",
    "    print(res)\n",
    "    \n",
    "# copied from fastai.structured\n",
    "def draw_tree(t, df, size=10, ratio=0.6, precision=0):\n",
    "    \"\"\" Draws a representation of a random forest in IPython.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    t: The tree you wish to draw\n",
    "    df: The data used to train the tree. This is used to get the names of the features.\n",
    "    \"\"\"\n",
    "    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n",
    "                      special_characters=True, rotate=True, precision=precision)\n",
    "    IPython.display.display(graphviz.Source(re.sub('Tree {',\n",
    "       f'Tree {{ size={size}; ratio={ratio}', s)))\n",
    "\n",
    "# copied from fastai.structured\n",
    "# fi = rf_feat_importance(RF,enc_df) -- NOT USED\n",
    "def rf_feat_importance(m, df):\n",
    "    return pd.DataFrame({'cols':df.columns, 'descrip':df.columns.to_series().map(feat_desc_dict),'importance':m.feature_importances_}\n",
    "                       ).sort_values('importance', ascending=False)\n",
    "def plot_fi(finew):\n",
    "    return finew.plot('Features','Importance','barh',figsize=(15,15),legend=False)\n",
    "\n",
    "## DESCRIPTION OF FEATURES - DICTIONARY\n",
    "def feature_desc():\n",
    "    feat_desc_dict = dict([(k,v) for k,v in zip(feat_desc.UDI,feat_desc.Description)])  \n",
    "    feat_unique_val = []\n",
    "    feat_unique_val.append(feat_desc_dict['eid'])\n",
    "    for i in range(1,len(feat_unique)):\n",
    "        for key, value in feat_desc_dict.items():   # iter on both keys and values\n",
    "            if key.startswith(feat_unique[i]+'-'):\n",
    "                feat_unique_val.append(value)\n",
    "                break\n",
    "    feat_unique_val = np.asarray(feat_unique_val)\n",
    "    feat_unique_dict = dict(zip(feat_unique, feat_unique_val))\n",
    "    \n",
    "    return feat_desc_dict, feat_unique_dict\n",
    "\n",
    "## IMPORTANCE OF FEATURES - DICTIONARY\n",
    "def feature_imp(RF, enc_df):    \n",
    "    imp = RF.feature_importances_.tolist()\n",
    "    col = enc_df.columns.values.tolist()\n",
    "    feat_imp_dict = dict(zip(col, imp))\n",
    "\n",
    "    feat_unique_impval = []\n",
    "    if(feat_unique[0] in feat_imp_dict.keys()):\n",
    "        feat_unique_impval.append(feat_imp_dict[feat_unique[0]])\n",
    "    else:\n",
    "        feat_unique_impval.append(0)\n",
    "\n",
    "    for i in range(1,len(feat_unique)):\n",
    "        temp = []\n",
    "        for key, value in feat_imp_dict.items():   # iter on both keys and values\n",
    "            if key.startswith(feat_unique[i]+'-'):\n",
    "                temp.append(feat_imp_dict[key])\n",
    "        if(len(temp) > 0):\n",
    "            feat_unique_impval.append(max(temp))\n",
    "        else:\n",
    "            feat_unique_impval.append(0)\n",
    "\n",
    "    feat_unique_imp_dict = dict(zip(feat_unique, feat_unique_impval))\n",
    "    \n",
    "    return feat_imp_dict, feat_unique_imp_dict\n",
    "\n",
    "def get_imp_features(num): # returns the dataframe of features along with their importances\n",
    "    feat = []\n",
    "    imp = []\n",
    "    imp_sorted = sorted(feat_unique_imp_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    for i in range(num):\n",
    "        feat.append(feat_unique_dict[imp_sorted[i][0]])\n",
    "        imp.append(imp_sorted[i][1])\n",
    "    \n",
    "    return pd.DataFrame(list(zip(feat, imp)), \n",
    "               columns =['Features', 'Importance'])\n",
    "\n",
    "\n",
    "\n",
    "# Plot the training and validation loss + accuracy\n",
    "def plot_training(history):\n",
    "    # summarize history for accuracy  \n",
    "\n",
    "    plt.subplot(211)  \n",
    "    plt.plot(history.history['accuracy'])  \n",
    "    plt.plot(history.history['val_accuracy'])  \n",
    "    plt.title('model accuracy')  \n",
    "    plt.ylabel('accuracy')  \n",
    "    plt.xlabel('epoch')  \n",
    "    plt.legend(['train', 'test'], loc='upper left')  \n",
    "\n",
    "    # summarize history for loss  \n",
    "\n",
    "    plt.subplot(212)  \n",
    "    plt.plot(history.history['loss'])  \n",
    "    plt.plot(history.history['val_loss'])  \n",
    "    plt.title('model loss')  \n",
    "    plt.ylabel('loss')  \n",
    "    plt.xlabel('epoch')  \n",
    "    plt.legend(['train', 'test'], loc='upper left')  \n",
    "    plt.show()  \n",
    "    \n",
    "\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] \n",
    "\n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > p)\n",
    "    normed_cm = (cm.T / cm.astype(np.float).sum(axis=1)).T\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(normed_cm, annot=True, fmt=\"f\",cbar=True, cmap=\"Blues\")\n",
    "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    print('Healthy Patients - correctly identified (True Negatives): ', cm[0][0], normed_cm[0][0])\n",
    "    print('Healthy Patients - predicted to have glaucoma (False Positives): ', cm[0][1], normed_cm[0][1])\n",
    "    print('Glaucoma Patients - predicted healthy (False Negatives): ', cm[1][0], normed_cm[1][0])\n",
    "    print('Glaucoma Patients - correctly identified (True Positives): ', cm[1][1], normed_cm[1][1])\n",
    "    print('\\nTotal Glaucoma patients: ', np.sum(cm[1]), np.sum(normed_cm[1]))\n",
    "    print('Total Healthy patients: ', np.sum(cm[0]), np.sum(normed_cm[0]))\n",
    "\n",
    "\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    heatmap = sns.heatmap(df_cm, annot=True, fmt=\"f\", cmap=\"Blues\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def plot_metrics(history):\n",
    "    metrics =  ['loss','Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n",
    "    \n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        \n",
    "        plt.subplot(3,2,n+1)\n",
    "        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "                 color=colors[0], linestyle=\"--\", label='Validation')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "#         if metric == 'loss':\n",
    "#             plt.ylim([0, plt.ylim()[1]])\n",
    "#         else:\n",
    "        plt.ylim([0,1])\n",
    "        plt.legend()\n",
    "        \n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = roc_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "def plot_loss(history, label, n):\n",
    "    # Use a log scale to show the wide range of values.\n",
    "    plt.semilogy(history.epoch,  history.history['loss'],\n",
    "               color=colors[n], label='Train '+label)\n",
    "    plt.semilogy(history.epoch,  history.history['val_loss'],\n",
    "          color=colors[n], label='Val '+label,\n",
    "          linestyle=\"--\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "def get_ROI(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # Find contour and sort by contour area\n",
    "    cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    if(len(cnts)>0):\n",
    "        cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "        # Find bounding box and extract ROI\n",
    "        for c in cnts:\n",
    "            x,y,w,h = cv2.boundingRect(c)\n",
    "            ROI = image[y:y+h, x:x+w]\n",
    "            break\n",
    "    else:\n",
    "        ROI = image\n",
    "        \n",
    "    return ROI\n",
    "\n",
    "def Recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def Precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def F1(y_true, y_pred):\n",
    "    precision = Precision(y_true, y_pred)\n",
    "    recall = Recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def get_metrics(y_train, y_val, y_test, train_pred, val_pred, test_pred):\n",
    "    print(\"Metrics:\\t\\tTrain:\\t\\t\\tValidation:\\t\\tTest:\")\n",
    "    print(\"Accuracy:\\t\",accuracy_score(y_train, train_pred), \"\\t\", accuracy_score(y_val, val_pred), \"\\t\", accuracy_score(y_test, test_pred))\n",
    "    print(\"Recall:\\t\\t\",recall_score(y_train, train_pred), \"\\t\", recall_score(y_val, val_pred), \"\\t\", recall_score(y_test, test_pred))\n",
    "    print(\"Precision:\\t\",precision_score(y_train, train_pred), \"\\t\", precision_score(y_val, val_pred), \"\\t\", precision_score(y_test, test_pred))\n",
    "    print(\"F1 score:\\t\",f1_score(y_train, train_pred), \"\\t\", f1_score(y_val, val_pred), \"\\t\", f1_score(y_test, test_pred))\n",
    "\n",
    "    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, train_pred)\n",
    "    fpr_val, tpr_val, thresholds_val = roc_curve(y_val, val_pred)\n",
    "    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, test_pred)\n",
    "    print(\"AUC:\\t\\t\",auc(fpr_train, tpr_train), \"\\t\", auc(fpr_val, tpr_val), \"\\t\", auc(fpr_test, tpr_test))\n",
    "    \n",
    "def vgg_model():       \n",
    "    vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(red_size, red_size, 3))\n",
    "\n",
    "    # Freeze the layers except the last 4 layers\n",
    "    for layer in vgg_conv.layers[:-4]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Create the model\n",
    "    model = models.Sequential()\n",
    "     # Add the vgg convolutional base model\n",
    "    model.add(vgg_conv)\n",
    " \n",
    "    # Add new layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu',kernel_regularizer=l2(reg_value)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    return model\n",
    "\n",
    "def resnet50_model():\n",
    "    resnet_model = resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=(red_size, red_size, 3))\n",
    "    # UnFreeze last conv layer\n",
    "    for layer in resnet_model.layers[:158]:\n",
    "        layer.trainable = False\n",
    "    for layer in resnet_model.layers[158:]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "     # Create the model\n",
    "    model = models.Sequential()\n",
    "    model.add(resnet_model)\n",
    " \n",
    "    # Add new layers\n",
    "    # https://adventuresinmachinelearning.com/global-average-pooling-convolutional-neural-networks/\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    return model\n",
    "\n",
    "def mobilenet_model():       \n",
    "    mobile_conv = MobileNet(weights='imagenet', include_top=False, input_shape=(red_size, red_size, 3))\n",
    "\n",
    "    # UnFreeze last conv layer\n",
    "    for layer in mobile_conv.layers[:78]:\n",
    "        layer.trainable = False\n",
    "    for layer in mobile_conv.layers[78:]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    # Create the model\n",
    "    model = models.Sequential()\n",
    "    model.add(mobile_conv)\n",
    " \n",
    "    # Add new layers\n",
    "    # https://adventuresinmachinelearning.com/global-average-pooling-convolutional-neural-networks/\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    return model\n",
    "\n",
    "def save_mlflow(keras_model_path,exp_name,which_gt,which_model,seq,BATCH_SIZE,n_epochs,n_steps_per_epoch,n_validation_steps,reg_value,learning_rate,decay,\n",
    "                opt,history,results_tr,results_vl,train_cm,val_cm):\n",
    "    # Create Experiment\n",
    "    exp_id = mlflow.create_experiment(exp_name)\n",
    "\n",
    "    # Start run as a child of that experiment\n",
    "    mlflow.start_run(experiment_id=exp_id)\n",
    "\n",
    "    # Save parameters\n",
    "    mlflow.log_param(\"which_gt\", which_gt)\n",
    "    mlflow.log_param(\"which_model\", which_model)\n",
    "    mlflow.log_param(\"augmentation\", seq)\n",
    "    mlflow.log_param(\"BATCH_SIZE\", BATCH_SIZE)\n",
    "    mlflow.log_param(\"epochs\", n_epochs)\n",
    "    mlflow.log_param(\"steps_per_epoch\", n_steps_per_epoch)\n",
    "    mlflow.log_param(\"validation_steps\", n_validation_steps)\n",
    "    mlflow.log_param(\"reg_value\", reg_value)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"decay\", decay)\n",
    "    mlflow.log_param(\"optimizer\", opt)\n",
    "    \n",
    "    # Save Model\n",
    "    mlflow.keras.save_model(model, keras_model_path)\n",
    "\n",
    "    # Save Metrics\n",
    "    for i in history.history.keys():\n",
    "        for j in range(len(history.history[i])):\n",
    "            mlflow.log_metric(i, history.history[i][j], step=j)\n",
    "\n",
    "    for i in ['results_tr','results_vl','train_cm','val_cm']:\n",
    "        for j in range(len(eval(i))):\n",
    "            mlflow.log_metric(i, eval(i)[j], step=j)\n",
    "            \n",
    "    mlflow.end_run()\n",
    "    \n",
    "def get_notebook_name():\n",
    "    from IPython.core.display import Javascript\n",
    "    from IPython.display import display\n",
    "\n",
    "    display(Javascript('Jupyter.notebook.kernel.execute(\\\"this_notebook = \" + \"\\'\"+Jupyter.notebook.notebook_name+\"\\'\");'))\n",
    "\n",
    "    # os.path.abspath(this_notebook)\n",
    "    return this_notebook\n",
    "\n",
    "\n",
    "print(\"Function definitions executed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From the file names, extracting the image data in array and respective labels\n",
    "# def get_data_labels(path, filenames,labels):\n",
    "#     n = len(filenames)\n",
    "#     indices = np.arange(n)\n",
    "#     np.random.shuffle(indices)\n",
    "#     tempdata = []\n",
    "#     templabels = []\n",
    "    \n",
    "#     for idx in range(500):\n",
    "#         img_name = filenames[colname].iloc[idx]\n",
    "#         label = np.asarray(labels.iloc[idx])\n",
    "        \n",
    "#         if (os.path.isfile(path+img_name)):\n",
    "#             orig_img = imageio.imread(path+img_name)\n",
    "#         else:\n",
    "#             orig_img = np.uint8(np.zeros((img_size,img_size,3)))\n",
    "        \n",
    "#         img = np.resize(orig_img,(224,224,3)) \n",
    "#         tempdata.append(img)\n",
    "#         templabels.append(label)\n",
    "        \n",
    "#     return np.asarray(tempdata), np.asarray(templabels)\n",
    "\n",
    "# # https://www.kaggle.com/aakashnain/beating-everything-with-depthwise-convolution\n",
    "# def data_gen(path, data, labels, batch_size):\n",
    "#     n = len(data)\n",
    "#     steps = n//batch_size\n",
    "#     colname = data.columns[0]\n",
    "  \n",
    "#     # Get a numpy array of all the indices of the input data\n",
    "#     indices = np.arange(n)\n",
    "    \n",
    "#     # Initialize a counter\n",
    "#     i = 0\n",
    "#     while True:\n",
    "#         np.random.shuffle(indices)\n",
    "#         # Get the next batch \n",
    "#         count = 0\n",
    "#         next_batch = indices[(i*batch_size):(i+1)*batch_size]\n",
    "        \n",
    "#         # Define two numpy arrays for containing batch data and labels\n",
    "#         batch_data = []\n",
    "#         batch_labels = []\n",
    "        \n",
    "#         for j, idx in enumerate(next_batch):\n",
    "#             img_name = data[colname].iloc[idx]\n",
    "#             label = np.asarray(labels.iloc[idx])\n",
    "#             if (os.path.isfile(path+img_name)):\n",
    "#                 orig_img = imageio.imread(path+img_name)\n",
    "#             else:\n",
    "#                 orig_img = np.uint8(np.zeros((img_size,img_size,3)))\n",
    "            \n",
    "#             if(len(orig_img.shape)==2):\n",
    "#                 orig_img = np.dstack([orig_img]*3)\n",
    "#                 print(orig_img.shape)\n",
    "            \n",
    "#             batch_data.append(orig_img)\n",
    "#             batch_labels.append(label)\n",
    "            \n",
    "#             count+=1\n",
    "            \n",
    "#             if(count == batch_size):\n",
    "#                 break\n",
    "        \n",
    "#         # end of for loop\n",
    "        \n",
    "#         aug_batch_data = seq.augment_images(batch_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         i+=1\n",
    "# #         print(\"Yielded:\", i, next_batch )\n",
    "#         yield np.asarray(aug_batch_data), np.asarray(batch_labels)\n",
    "        \n",
    "#         if (i>=steps):\n",
    "#             print(i)\n",
    "#             i = 0\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Book Creation\n",
    "\n",
    "\n",
    "**disease_types** ['eye','neuro','cardio','systemic'] as keys and disease codes belonging to that category as values\n",
    "\n",
    "**surgery_types** same keys as above with surgery/disease codes belonging to that category as values\n",
    "\n",
    "**disease_codebook** disease code as keys and description as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Coding 6\n",
    "k = ['eye','neuro','cardio','systemic']\n",
    "v = [[1076,1265,1268,1300,1301,1302,1303,1304,1305,1307,1308,1490,1590,1591,1592,1593,1681],\n",
    "    [1076,1266,1270,1271,1272,1273,1274,1275,1276,1277,1278,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1438,1442,1488,1489,1491,1492,1528,1586,1587,1588,1589,1733,1779],\n",
    "    [1065,1066,1067,1071,1081,1082,1083,1084,1085,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1103,1104,1105,1111,1112,1480,1481,1532,1536,1542,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659],\n",
    "    [1075,1080,1245,1246,1247,1248,1414,1415,1416,1417,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1543,1544,1545,1584,1625,1626,1627,1628,1642,1643]]\n",
    "disease_types = {k[i]: v[i] for i in range(len(k))}\n",
    "\n",
    "# Data Coding 5\n",
    "v1 = [[1076,1481,1482,1483,1484,1548,1651,1652],\n",
    "     [1076,1521,1522,1523,1524,1525,1526,1527,1528],\n",
    "     [1071,1086,1087,1088,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1127,1128,1129,1437,1488,1489,1490,1491,1492,1531,1535,1581,1582,1608,1609,1610,1611,1613,1614,1615],\n",
    "     []]\n",
    "surgery_types = {k[i]: v1[i] for i in range(len(k))}\n",
    "\n",
    "df = pd.read_excel(DATA_dir + 'disease_codebook.xlsx')\n",
    "disease_codebook = dict([(k,v) for k,v in zip(df.CODE,df.DISEASE)])\n",
    "disease_codebook[0] = 'nan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Latest Disease codebook by ICD 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Data Coding 19 from UK Bio Bank Dataset\n",
    "\n",
    "# Creating disease codebook for latest disease list by ICD10\n",
    "# xls = ExcelFile(DATA_dir + 'ICD10_DC19.xlsx')\n",
    "# df = xls.parse(xls.sheet_names[0])\n",
    "df = pd.read_excel(DATA_dir + 'ICD10_DC19.xlsx')\n",
    "\n",
    "# This is a dictionary having the disease name/ meaning for each code defined. key: code, value: meaning\n",
    "icd10 = dict([(key,val) for key,val in zip(df.coding,df.meaning)])\n",
    "discode_list = df[\"coding\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EYE\n",
    "sub_eye = []\n",
    "for i in range(7):\n",
    "    sub_eye.append('H0'+str(i))\n",
    "for i in range(10,14):\n",
    "    sub_eye.append('H'+str(i))   \n",
    "for i in range(15,23):\n",
    "    sub_eye.append('H'+str(i))\n",
    "for i in range(25,29):\n",
    "    sub_eye.append('H'+str(i))\n",
    "for i in range(30,37):\n",
    "    sub_eye.append('H'+str(i))    \n",
    "for i in range(40,43):\n",
    "    sub_eye.append('H'+str(i))    \n",
    "for i in range(43,46):\n",
    "    sub_eye.append('H'+str(i))    \n",
    "for i in range(46,49):\n",
    "    sub_eye.append('H'+str(i))    \n",
    "for i in range(49,53):\n",
    "    sub_eye.append('H'+str(i))    \n",
    "for i in range(53,55):\n",
    "    sub_eye.append('H'+str(i))    \n",
    "for i in range(55,60):\n",
    "    sub_eye.append('H'+str(i))\n",
    "\n",
    "\n",
    "# NEURO\n",
    "sub_neuro = []\n",
    "for i in range(10):\n",
    "    sub_neuro.append('G0'+str(i))\n",
    "for i in range(10,15):\n",
    "    sub_neuro.append('G'+str(i))   \n",
    "for i in range(20,27):\n",
    "    sub_neuro.append('G'+str(i))\n",
    "for i in range(30,33):\n",
    "    sub_neuro.append('G'+str(i))\n",
    "for i in range(35,38):\n",
    "    sub_neuro.append('G'+str(i))    \n",
    "for i in range(40,48):\n",
    "    sub_neuro.append('G'+str(i))    \n",
    "for i in range(50,60):\n",
    "    sub_neuro.append('G'+str(i))    \n",
    "for i in range(60,65):\n",
    "    sub_neuro.append('G'+str(i))    \n",
    "for i in range(70,74):\n",
    "    sub_neuro.append('G'+str(i))    \n",
    "for i in range(80,84):\n",
    "    sub_neuro.append('G'+str(i))    \n",
    "for i in range(90,100):\n",
    "    sub_neuro.append('G'+str(i))\n",
    "    \n",
    "\n",
    "# CARDIO\n",
    "sub_cardio = []\n",
    "for i in range(3):\n",
    "    sub_cardio.append('I0'+str(i))\n",
    "for i in range(5,10):\n",
    "    sub_cardio.append('I'+str(i))   \n",
    "for i in range(10,16):\n",
    "    sub_cardio.append('I'+str(i))\n",
    "for i in range(20,26):\n",
    "    sub_cardio.append('I'+str(i))\n",
    "for i in range(26,29):\n",
    "    sub_cardio.append('I'+str(i))    \n",
    "for i in range(30,53):\n",
    "    sub_cardio.append('I'+str(i))    \n",
    "for i in range(60,70):\n",
    "    sub_cardio.append('I'+str(i))    \n",
    "for i in range(70,80):\n",
    "    sub_cardio.append('I'+str(i))    \n",
    "for i in range(80,90):\n",
    "    sub_cardio.append('I'+str(i))    \n",
    "for i in range(95,100):\n",
    "    sub_cardio.append('I'+str(i))\n",
    "\n",
    "sub = [] # list of 3 lists -eye, neuro, cardio\n",
    "sub.append(sub_eye)\n",
    "sub.append(sub_neuro)\n",
    "sub.append(sub_cardio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mykey = ['EYE','NEURO','CARDIO']\n",
    "myval = [[],[],[]]\n",
    "\n",
    "for sub_type in range(3):\n",
    "    for i in discode_list:\n",
    "        for j in sub[sub_type]:\n",
    "            if(str(j) in str(i)):\n",
    "                myval[sub_type].append(str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dc19_categories is a dictionary having keys as ['EYE','NEURO','CARDIO'] and their respective diseases as the values in a list.\n",
    "dc19_categories = {mykey[i]: myval[i] for i in range(len(mykey))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_eye_dict = {0: \"Don't know\", 1: \"Left Eye\", 2: \"Right Eye\", 3: \"Both Eyes\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_desc = pd.read_excel(DATA_dir + 'FeatureNames.xlsx')\n",
    "feat_desc_dict = dict([(k,v) for k,v in zip(feat_desc.UDI,feat_desc.Description)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definitions DONE !!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Definitions DONE !!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # experiment With Reg params \n",
    "\n",
    "# # grid search values\n",
    "# # all_hist, ev_train, ev_val, pr_train, pr_val = list(), list(), list(), list(), list()\n",
    "\n",
    "# opt = Adam(lr=1e-3) #lr=0.0001 , decay=1e-5\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=2, min_lr=0.000001, verbose=1)\n",
    "\n",
    "# for param in values:\n",
    "#     print(\"Creating model with param:\",param)\n",
    "#     reg_model = make_model_regtest(param)\n",
    "#     reg_model.compile(loss='binary_crossentropy', metrics=METRICS, optimizer=opt)\n",
    "\n",
    "#     print(\"Model.fit\")\n",
    "#     hist = reg_model.fit_generator(train_generator, \n",
    "#                               epochs=3, \n",
    "#                               steps_per_epoch=10,\n",
    "#                               validation_data=validation_generator,\n",
    "#                               validation_steps=10, \n",
    "#                               callbacks=[reduce_lr]\n",
    "#                              )\n",
    "#     print(\"Train Model evaluation\")\n",
    "#     res_tr_reg = reg_model.evaluate(train_generator)\n",
    "#     print(\"Validation Model evaluation\")\n",
    "#     res_vl_reg = reg_model.evaluate(validation_generator)\n",
    "\n",
    "#     print(\"Train model prediction\")\n",
    "#     pred_tr_reg = reg_model.predict(train_generator)\n",
    "#     print(\"Validation Model prediction\")\n",
    "#     pred_vl_reg = reg_model.predict(validation_generator)\n",
    "    \n",
    "#     all_hist.append(hist)\n",
    "#     ev_train.append(res_tr_reg)\n",
    "#     ev_val.append(res_vl_reg)\n",
    "#     pr_train.append(pred_tr_reg)\n",
    "#     pr_val.append(pred_vl_reg)\n",
    "    \n",
    "# # # plot train and test means\n",
    "# # from matplotlib import pyplot\n",
    "\n",
    "# # pyplot.semilogx(values, all_train, label='train', marker='g')\n",
    "# # pyplot.semilogx(values, all_val, label='validation', marker='o')\n",
    "# # pyplot.legend()\n",
    "# # pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2020",
   "language": "python",
   "name": "python2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
